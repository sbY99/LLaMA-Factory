{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCM 데이터셋을 LLaMA Factory 형식에 맞게 전처리\n",
    "\n",
    "이 노트북은 DCM 환자 데이터를 LLaMA Factory의 ShareGPT 형식으로 변환합니다.\n",
    "3가지 예측 모드별로 별도의 JSON 파일을 생성합니다:\n",
    "- delta_only: JOA score 변화 예측\n",
    "- postop_only: 수술 후 JOA score 예측\n",
    "- binary_only: 환자 개선 여부 예측 (회복률 >= 60%)\n",
    "\n",
    "**핵심 특징**: 이 노트북은 데이터 로딩과 파일 I/O만 처리하며,\n",
    "**모든 프롬프트 관련 로직은 `src/prompts.py`에서 가져옵니다.**\n",
    "\n",
    "따라서 프롬프트를 수정하려면:\n",
    "1. `src/prompts.py` 편집\n",
    "2. 이 노트북 재실행\n",
    "3. 변경사항이 자동으로 반영됨!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "import sys\n",
    "\n",
    "# Add src directory to path for importing prompts\n",
    "sys.path.append('../../src')\n",
    "\n",
    "# Import ALL prompt-related functions from src/prompts.py\n",
    "from prompts import (\n",
    "    get_system_prompt,\n",
    "    construct_user_prompt_sharegpt,\n",
    "    create_ground_truth_response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: /home/seungbinyang/dcm_joa_prediction/LLaMA-Factory/data/../../data\n",
      "Preprocessed directory: /home/seungbinyang/dcm_joa_prediction/LLaMA-Factory/data/../../data/preprocessed\n",
      "Output directory: /home/seungbinyang/dcm_joa_prediction/LLaMA-Factory/data\n"
     ]
    }
   ],
   "source": [
    "# Paths configuration\n",
    "DATA_DIR = Path(\"../../data\")\n",
    "PREPROCESSED_DIR = DATA_DIR / \"preprocessed\"\n",
    "PATIENT_LIST_PATH = DATA_DIR / \"patient_list.txt\"\n",
    "METADATA_PATH = DATA_DIR / \"metadata\" / \"tabular_250520.csv\"\n",
    "TABULAR_VARIABLES_PATH = DATA_DIR / \"tabular_variables.txt\"\n",
    "\n",
    "# Output paths (relative to LLaMA-Factory/data/)\n",
    "OUTPUT_DIR = Path(\".\")\n",
    "OUTPUT_FILES = {\n",
    "    \"delta_only\": OUTPUT_DIR / \"dcm_delta_only.json\",\n",
    "    \"postop_only\": OUTPUT_DIR / \"dcm_postop_only.json\",\n",
    "    \"binary_only\": OUTPUT_DIR / \"dcm_binary_only.json\"\n",
    "}\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR.absolute()}\")\n",
    "print(f\"Preprocessed directory: {PREPROCESSED_DIR.absolute()}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 387 patients\n",
      "Loaded 15 allowed tabular variables\n",
      "Loaded metadata for 365 patients\n"
     ]
    }
   ],
   "source": [
    "def load_patient_list(patient_list_path: Path) -> List[str]:\n",
    "    \"\"\"Load patient list from text file.\"\"\"\n",
    "    with open(patient_list_path, 'r') as f:\n",
    "        patients = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Loaded {len(patients)} patients\")\n",
    "    return patients\n",
    "\n",
    "def load_allowed_tabular_variables(tabular_vars_path: Path) -> List[str]:\n",
    "    \"\"\"Load allowed tabular variables.\"\"\"\n",
    "    with open(tabular_vars_path, 'r') as f:\n",
    "        variables = [line.strip() for line in f if line.strip()]\n",
    "    print(f\"Loaded {len(variables)} allowed tabular variables\")\n",
    "    return variables\n",
    "\n",
    "def filter_tabular_data(tabular_content: str, allowed_variables: List[str]) -> str:\n",
    "    \"\"\"Filter tabular data to only include allowed variables.\"\"\"\n",
    "    pairs = [p.strip() for p in tabular_content.split(',')]\n",
    "    filtered_pairs = []\n",
    "    for pair in pairs:\n",
    "        if '=' in pair:\n",
    "            var_name = pair.split('=')[0].strip()\n",
    "            if var_name in allowed_variables:\n",
    "                filtered_pairs.append(pair)\n",
    "    return ', '.join(filtered_pairs)\n",
    "\n",
    "# Load data\n",
    "patient_list = load_patient_list(PATIENT_LIST_PATH)\n",
    "allowed_tabular_vars = load_allowed_tabular_variables(TABULAR_VARIABLES_PATH)\n",
    "metadata_df = pd.read_csv(METADATA_PATH)\n",
    "metadata_df['CaseNum'] = metadata_df['CaseNum'].astype(str)\n",
    "metadata_df = metadata_df[metadata_df['CaseNum'].isin(patient_list)].copy()\n",
    "print(f\"Loaded metadata for {len(metadata_df)} patients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading functions defined\n"
     ]
    }
   ],
   "source": [
    "def get_t2s_image_paths(patient_dir: Path) -> List[str]:\n",
    "    \"\"\"Get T2S image paths (6 slices in sorted order).\"\"\"\n",
    "    t2s_dir = patient_dir / \"MR_T2S\"\n",
    "    if not t2s_dir.exists():\n",
    "        return []\n",
    "    \n",
    "    png_files = sorted([f for f in t2s_dir.iterdir() if f.suffix.lower() == '.png'])\n",
    "    if len(png_files) != 6:\n",
    "        return []\n",
    "    \n",
    "    # Return relative paths from LLaMA-Factory/data/ directory\n",
    "    return [f\"../../data/preprocessed/{patient_dir.name}/MR_T2S/{f.name}\" for f in png_files]\n",
    "\n",
    "def get_t2a_image_paths(patient_dir: Path) -> List[str]:\n",
    "    \"\"\"Get T2A image paths (top 6 most stenotic slices based on ranking).\"\"\"\n",
    "    t2a_dir = patient_dir / \"MR_T2A\"\n",
    "    ranking_path = patient_dir / \"MR_T2A_ranking.json\"\n",
    "    \n",
    "    if not t2a_dir.exists() or not ranking_path.exists():\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(ranking_path, 'r') as f:\n",
    "            ranking_data = json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading ranking file for {patient_dir.name}: {e}\")\n",
    "        return []\n",
    "    \n",
    "    if len(ranking_data) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Sort by rank (ascending) and select top 6 most stenotic slices\n",
    "    sorted_by_rank = sorted(ranking_data, key=lambda x: x['rank'])\n",
    "    top_6 = sorted_by_rank[:6]\n",
    "    \n",
    "    # Build relative file paths\n",
    "    image_paths = []\n",
    "    for slice_info in top_6:\n",
    "        slice_path = t2a_dir / slice_info['filename']\n",
    "        if not slice_path.exists():\n",
    "            return []\n",
    "        image_paths.append(f\"../../data/preprocessed/{patient_dir.name}/MR_T2A/{slice_info['filename']}\")\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "def load_tabular_data(patient_dir: Path, allowed_vars: List[str]) -> str:\n",
    "    \"\"\"Load and filter tabular data.\"\"\"\n",
    "    tabular_path = patient_dir / \"tabular_data_note.txt\"\n",
    "    if not tabular_path.exists() or tabular_path.stat().st_size == 0:\n",
    "        return \"Not Provided\"\n",
    "    \n",
    "    try:\n",
    "        content = tabular_path.read_text().strip()\n",
    "        if not content or content in [\"Not Available\", \"Error Reading File\"]:\n",
    "            return \"Not Provided\"\n",
    "        \n",
    "        # Filter to allowed variables\n",
    "        filtered = filter_tabular_data(content, allowed_vars)\n",
    "        return filtered if filtered else \"Not Provided\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading tabular data for {patient_dir.name}: {e}\")\n",
    "        return \"Not Provided\"\n",
    "\n",
    "def load_text_reports(patient_dir: Path) -> Dict[str, str]:\n",
    "    \"\"\"Load text reports (MR/XR reports, admission/operative notes).\"\"\"\n",
    "    text_files = {\n",
    "        \"MR_REPORT\": patient_dir / \"MR_report.txt\",\n",
    "        \"XR_REPORT\": patient_dir / \"XR_report.txt\",\n",
    "        \"ADMISSION_NOTE\": patient_dir / \"admission_note.txt\",\n",
    "        \"OPERATIVE_NOTE\": patient_dir / \"operative_note.txt\"\n",
    "    }\n",
    "    \n",
    "    valid_reports = {}\n",
    "    for key, file_path in text_files.items():\n",
    "        if file_path.exists() and file_path.stat().st_size > 0:\n",
    "            try:\n",
    "                content = file_path.read_text().strip()\n",
    "                if content and content not in [\"Not Available\", \"Error Reading File\"]:\n",
    "                    valid_reports[key] = content\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {key} for {patient_dir.name}: {e}\")\n",
    "    \n",
    "    return valid_reports\n",
    "\n",
    "print(\"Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patient processing function defined\n"
     ]
    }
   ],
   "source": [
    "def process_patient_for_target(patient_id: str, patient_metadata: pd.Series, \n",
    "                               prediction_target: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Process a single patient and create dataset entry for specific prediction target.\n",
    "    \n",
    "    This function:\n",
    "    1. Loads patient data (images, tabular, text)\n",
    "    2. Uses src/prompts.py functions to generate prompts:\n",
    "       - get_system_prompt() for system message\n",
    "       - construct_user_prompt_sharegpt() for user message\n",
    "       - create_ground_truth_response() for assistant message\n",
    "    3. Returns ShareGPT format entry\n",
    "    \n",
    "    All prompt text and logic comes from src/prompts.py!\n",
    "    \"\"\"\n",
    "    \n",
    "    patient_dir = PREPROCESSED_DIR / patient_id\n",
    "    if not patient_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    # Load all patient data\n",
    "    t2s_paths = get_t2s_image_paths(patient_dir)\n",
    "    t2a_paths = get_t2a_image_paths(patient_dir)\n",
    "    tabular_data = load_tabular_data(patient_dir, allowed_tabular_vars)\n",
    "    text_reports = load_text_reports(patient_dir)\n",
    "    \n",
    "    # Validate required data\n",
    "    if not t2s_paths or not t2a_paths:\n",
    "        print(f\"Skipping patient {patient_id}: Missing required image data\")\n",
    "        return None\n",
    "    \n",
    "    # === ALL PROMPTS FROM src/prompts.py ===\n",
    "    \n",
    "    # System prompt from src/prompts.py\n",
    "    system_content = get_system_prompt(prediction_target)\n",
    "    \n",
    "    # User prompt from src/prompts.py\n",
    "    image_paths_dict = {\n",
    "        \"MR_T2S\": t2s_paths,\n",
    "        \"MR_T2A\": t2a_paths\n",
    "    }\n",
    "    modality_keys = [\"MR_T2S\", \"MR_T2A\", \"tabular\", \"text\"]\n",
    "    \n",
    "    user_content = construct_user_prompt_sharegpt(\n",
    "        image_paths_dict,\n",
    "        tabular_data,\n",
    "        text_reports,\n",
    "        modality_keys\n",
    "    )\n",
    "    \n",
    "    # Assistant response (ground truth) from src/prompts.py\n",
    "    assistant_content = create_ground_truth_response(\n",
    "        patient_metadata['preop_joa'],\n",
    "        patient_metadata['delta_joa'],\n",
    "        prediction_target\n",
    "    )\n",
    "    \n",
    "    # === END OF PROMPTS ===\n",
    "    \n",
    "    # Combine all image paths\n",
    "    all_image_paths = t2s_paths + t2a_paths\n",
    "    \n",
    "    # Create dataset entry in ShareGPT format\n",
    "    entry = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": system_content,\n",
    "                \"role\": \"system\"\n",
    "            },\n",
    "            {\n",
    "                \"content\": user_content,\n",
    "                \"role\": \"user\"\n",
    "            },\n",
    "            {\n",
    "                \"content\": assistant_content,\n",
    "                \"role\": \"assistant\"\n",
    "            }\n",
    "        ],\n",
    "        \"images\": all_image_paths\n",
    "    }\n",
    "    \n",
    "    return entry\n",
    "\n",
    "print(\"Patient processing function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Dataset Generation - Fully Integrated with src/prompts.py\n",
      "============================================================\n",
      "\n",
      "This notebook uses the following functions from src/prompts.py:\n",
      "  1. get_system_prompt() - System message\n",
      "  2. construct_user_prompt_sharegpt() - User message\n",
      "  3. create_ground_truth_response() - Assistant message\n",
      "\n",
      "NO prompt text is hardcoded in this notebook.\n",
      "To modify any prompt, edit src/prompts.py and re-run.\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Processing dataset for: delta_only\n",
      "============================================================\n",
      "Skipping patient 13: Missing required image data\n",
      "Skipping patient 27: Missing required image data\n",
      "Skipping patient 29: Missing required image data\n",
      "Skipping patient 50: Missing required image data\n",
      "Processed 50 patients...\n",
      "Skipping patient 71: Missing required image data\n",
      "Skipping patient 82: Missing required image data\n",
      "Skipping patient 83: Missing required image data\n",
      "Skipping patient 89: Missing required image data\n",
      "Skipping patient 94: Missing required image data\n",
      "Skipping patient 98: Missing required image data\n",
      "Skipping patient 108: Missing required image data\n",
      "Skipping patient 109: Missing required image data\n",
      "Processed 100 patients...\n",
      "Skipping patient 115: Missing required image data\n",
      "Skipping patient 130: Missing required image data\n",
      "Skipping patient 132: Missing required image data\n",
      "Skipping patient 135: Missing required image data\n",
      "Skipping patient 136: Missing required image data\n",
      "Skipping patient 138: Missing required image data\n",
      "Skipping patient 140: Missing required image data\n",
      "Skipping patient 142: Missing required image data\n",
      "Skipping patient 145: Missing required image data\n",
      "Skipping patient 147: Missing required image data\n",
      "Skipping patient 151: Missing required image data\n",
      "Skipping patient 154: Missing required image data\n",
      "Skipping patient 155: Missing required image data\n",
      "Skipping patient 161: Missing required image data\n",
      "Skipping patient 162: Missing required image data\n",
      "Skipping patient 172: Missing required image data\n",
      "Skipping patient 182: Missing required image data\n",
      "Processed 150 patients...\n",
      "Skipping patient 191: Missing required image data\n",
      "Skipping patient 196: Missing required image data\n",
      "Skipping patient 203: Missing required image data\n",
      "Skipping patient 206: Missing required image data\n",
      "Skipping patient 208: Missing required image data\n",
      "Skipping patient 210: Missing required image data\n",
      "Skipping patient 215: Missing required image data\n",
      "Skipping patient 218: Missing required image data\n",
      "Skipping patient 221: Missing required image data\n",
      "Skipping patient 223: Missing required image data\n",
      "Skipping patient 225: Missing required image data\n",
      "Skipping patient 227: Missing required image data\n",
      "Skipping patient 234: Missing required image data\n",
      "Skipping patient 236: Missing required image data\n",
      "Skipping patient 238: Missing required image data\n",
      "Skipping patient 240: Missing required image data\n",
      "Skipping patient 244: Missing required image data\n",
      "Skipping patient 247: Missing required image data\n",
      "Skipping patient 252: Missing required image data\n",
      "Skipping patient 256: Missing required image data\n",
      "Skipping patient 258: Missing required image data\n",
      "Processed 200 patients...\n",
      "Skipping patient 270: Missing required image data\n",
      "Skipping patient 271: Missing required image data\n",
      "Skipping patient 276: Missing required image data\n",
      "Skipping patient 280: Missing required image data\n",
      "Skipping patient 285: Missing required image data\n",
      "Skipping patient 295: Missing required image data\n",
      "Skipping patient 298: Missing required image data\n",
      "Skipping patient 301: Missing required image data\n",
      "Skipping patient 321: Missing required image data\n",
      "Skipping patient 331: Missing required image data\n",
      "Processed 250 patients...\n",
      "Skipping patient 335: Missing required image data\n",
      "Skipping patient 336: Missing required image data\n",
      "Skipping patient 337: Missing required image data\n",
      "Skipping patient 338: Missing required image data\n",
      "Skipping patient 339: Missing required image data\n",
      "Skipping patient 352: Missing required image data\n",
      "Skipping patient 353: Missing required image data\n",
      "Skipping patient 357: Missing required image data\n",
      "Skipping patient 359: Missing required image data\n",
      "Skipping patient 361: Missing required image data\n",
      "Skipping patient 365: Missing required image data\n",
      "Skipping patient 370: Missing required image data\n",
      "Skipping patient 378: Missing required image data\n",
      "Skipping patient 381: Missing required image data\n",
      "Skipping patient 385: Missing required image data\n",
      "Skipping patient 387: Missing required image data\n",
      "\n",
      "✓ Saved 287 samples to dcm_delta_only.json\n",
      "  Skipped 78 patients due to missing data\n",
      "\n",
      "  Sample entry structure:\n",
      "  - Number of images: 12\n",
      "  - Number of messages: 3\n",
      "  - System message length: 2356 chars\n",
      "  - User message length: 10888 chars\n",
      "  - Assistant response: {\"rationale\": \"Based on the provided imaging and clinical data, the predicted JOA change is -1.0.\", ...\n",
      "\n",
      "============================================================\n",
      "Processing dataset for: postop_only\n",
      "============================================================\n",
      "Skipping patient 13: Missing required image data\n",
      "Skipping patient 27: Missing required image data\n",
      "Skipping patient 29: Missing required image data\n",
      "Skipping patient 50: Missing required image data\n",
      "Processed 50 patients...\n",
      "Skipping patient 71: Missing required image data\n",
      "Skipping patient 82: Missing required image data\n",
      "Skipping patient 83: Missing required image data\n",
      "Skipping patient 89: Missing required image data\n",
      "Skipping patient 94: Missing required image data\n",
      "Skipping patient 98: Missing required image data\n",
      "Skipping patient 108: Missing required image data\n",
      "Skipping patient 109: Missing required image data\n",
      "Processed 100 patients...\n",
      "Skipping patient 115: Missing required image data\n",
      "Skipping patient 130: Missing required image data\n",
      "Skipping patient 132: Missing required image data\n",
      "Skipping patient 135: Missing required image data\n",
      "Skipping patient 136: Missing required image data\n",
      "Skipping patient 138: Missing required image data\n",
      "Skipping patient 140: Missing required image data\n",
      "Skipping patient 142: Missing required image data\n",
      "Skipping patient 145: Missing required image data\n",
      "Skipping patient 147: Missing required image data\n",
      "Skipping patient 151: Missing required image data\n",
      "Skipping patient 154: Missing required image data\n",
      "Skipping patient 155: Missing required image data\n",
      "Skipping patient 161: Missing required image data\n",
      "Skipping patient 162: Missing required image data\n",
      "Skipping patient 172: Missing required image data\n",
      "Skipping patient 182: Missing required image data\n",
      "Processed 150 patients...\n",
      "Skipping patient 191: Missing required image data\n",
      "Skipping patient 196: Missing required image data\n",
      "Skipping patient 203: Missing required image data\n",
      "Skipping patient 206: Missing required image data\n",
      "Skipping patient 208: Missing required image data\n",
      "Skipping patient 210: Missing required image data\n",
      "Skipping patient 215: Missing required image data\n",
      "Skipping patient 218: Missing required image data\n",
      "Skipping patient 221: Missing required image data\n",
      "Skipping patient 223: Missing required image data\n",
      "Skipping patient 225: Missing required image data\n",
      "Skipping patient 227: Missing required image data\n",
      "Skipping patient 234: Missing required image data\n",
      "Skipping patient 236: Missing required image data\n",
      "Skipping patient 238: Missing required image data\n",
      "Skipping patient 240: Missing required image data\n",
      "Skipping patient 244: Missing required image data\n",
      "Skipping patient 247: Missing required image data\n",
      "Skipping patient 252: Missing required image data\n",
      "Skipping patient 256: Missing required image data\n",
      "Skipping patient 258: Missing required image data\n",
      "Processed 200 patients...\n",
      "Skipping patient 270: Missing required image data\n",
      "Skipping patient 271: Missing required image data\n",
      "Skipping patient 276: Missing required image data\n",
      "Skipping patient 280: Missing required image data\n",
      "Skipping patient 285: Missing required image data\n",
      "Skipping patient 295: Missing required image data\n",
      "Skipping patient 298: Missing required image data\n",
      "Skipping patient 301: Missing required image data\n",
      "Skipping patient 321: Missing required image data\n",
      "Skipping patient 331: Missing required image data\n",
      "Processed 250 patients...\n",
      "Skipping patient 335: Missing required image data\n",
      "Skipping patient 336: Missing required image data\n",
      "Skipping patient 337: Missing required image data\n",
      "Skipping patient 338: Missing required image data\n",
      "Skipping patient 339: Missing required image data\n",
      "Skipping patient 352: Missing required image data\n",
      "Skipping patient 353: Missing required image data\n",
      "Skipping patient 357: Missing required image data\n",
      "Skipping patient 359: Missing required image data\n",
      "Skipping patient 361: Missing required image data\n",
      "Skipping patient 365: Missing required image data\n",
      "Skipping patient 370: Missing required image data\n",
      "Skipping patient 378: Missing required image data\n",
      "Skipping patient 381: Missing required image data\n",
      "Skipping patient 385: Missing required image data\n",
      "Skipping patient 387: Missing required image data\n",
      "\n",
      "✓ Saved 287 samples to dcm_postop_only.json\n",
      "  Skipped 78 patients due to missing data\n",
      "\n",
      "  Sample entry structure:\n",
      "  - Number of images: 12\n",
      "  - Number of messages: 3\n",
      "  - System message length: 2425 chars\n",
      "  - User message length: 10888 chars\n",
      "  - Assistant response: {\"rationale\": \"Based on the provided imaging and clinical data, the predicted post-operative JOA sco...\n",
      "\n",
      "============================================================\n",
      "Processing dataset for: binary_only\n",
      "============================================================\n",
      "Skipping patient 13: Missing required image data\n",
      "Skipping patient 27: Missing required image data\n",
      "Skipping patient 29: Missing required image data\n",
      "Skipping patient 50: Missing required image data\n",
      "Processed 50 patients...\n",
      "Skipping patient 71: Missing required image data\n",
      "Skipping patient 82: Missing required image data\n",
      "Skipping patient 83: Missing required image data\n",
      "Skipping patient 89: Missing required image data\n",
      "Skipping patient 94: Missing required image data\n",
      "Skipping patient 98: Missing required image data\n",
      "Skipping patient 108: Missing required image data\n",
      "Skipping patient 109: Missing required image data\n",
      "Processed 100 patients...\n",
      "Skipping patient 115: Missing required image data\n",
      "Skipping patient 130: Missing required image data\n",
      "Skipping patient 132: Missing required image data\n",
      "Skipping patient 135: Missing required image data\n",
      "Skipping patient 136: Missing required image data\n",
      "Skipping patient 138: Missing required image data\n",
      "Skipping patient 140: Missing required image data\n",
      "Skipping patient 142: Missing required image data\n",
      "Skipping patient 145: Missing required image data\n",
      "Skipping patient 147: Missing required image data\n",
      "Skipping patient 151: Missing required image data\n",
      "Skipping patient 154: Missing required image data\n",
      "Skipping patient 155: Missing required image data\n",
      "Skipping patient 161: Missing required image data\n",
      "Skipping patient 162: Missing required image data\n",
      "Skipping patient 172: Missing required image data\n",
      "Skipping patient 182: Missing required image data\n",
      "Processed 150 patients...\n",
      "Skipping patient 191: Missing required image data\n",
      "Skipping patient 196: Missing required image data\n",
      "Skipping patient 203: Missing required image data\n",
      "Skipping patient 206: Missing required image data\n",
      "Skipping patient 208: Missing required image data\n",
      "Skipping patient 210: Missing required image data\n",
      "Skipping patient 215: Missing required image data\n",
      "Skipping patient 218: Missing required image data\n",
      "Skipping patient 221: Missing required image data\n",
      "Skipping patient 223: Missing required image data\n",
      "Skipping patient 225: Missing required image data\n",
      "Skipping patient 227: Missing required image data\n",
      "Skipping patient 234: Missing required image data\n",
      "Skipping patient 236: Missing required image data\n",
      "Skipping patient 238: Missing required image data\n",
      "Skipping patient 240: Missing required image data\n",
      "Skipping patient 244: Missing required image data\n",
      "Skipping patient 247: Missing required image data\n",
      "Skipping patient 252: Missing required image data\n",
      "Skipping patient 256: Missing required image data\n",
      "Skipping patient 258: Missing required image data\n",
      "Processed 200 patients...\n",
      "Skipping patient 270: Missing required image data\n",
      "Skipping patient 271: Missing required image data\n",
      "Skipping patient 276: Missing required image data\n",
      "Skipping patient 280: Missing required image data\n",
      "Skipping patient 285: Missing required image data\n",
      "Skipping patient 295: Missing required image data\n",
      "Skipping patient 298: Missing required image data\n",
      "Skipping patient 301: Missing required image data\n",
      "Skipping patient 321: Missing required image data\n",
      "Skipping patient 331: Missing required image data\n",
      "Processed 250 patients...\n",
      "Skipping patient 335: Missing required image data\n",
      "Skipping patient 336: Missing required image data\n",
      "Skipping patient 337: Missing required image data\n",
      "Skipping patient 338: Missing required image data\n",
      "Skipping patient 339: Missing required image data\n",
      "Skipping patient 352: Missing required image data\n",
      "Skipping patient 353: Missing required image data\n",
      "Skipping patient 357: Missing required image data\n",
      "Skipping patient 359: Missing required image data\n",
      "Skipping patient 361: Missing required image data\n",
      "Skipping patient 365: Missing required image data\n",
      "Skipping patient 370: Missing required image data\n",
      "Skipping patient 378: Missing required image data\n",
      "Skipping patient 381: Missing required image data\n",
      "Skipping patient 385: Missing required image data\n",
      "Skipping patient 387: Missing required image data\n",
      "\n",
      "✓ Saved 287 samples to dcm_binary_only.json\n",
      "  Skipped 78 patients due to missing data\n",
      "\n",
      "  Sample entry structure:\n",
      "  - Number of images: 12\n",
      "  - Number of messages: 3\n",
      "  - System message length: 2893 chars\n",
      "  - User message length: 10888 chars\n",
      "  - Assistant response: {\"rationale\": \"Based on the provided imaging and clinical data, the patient is not expected to achie...\n",
      "\n",
      "============================================================\n",
      "All datasets generated successfully!\n",
      "All prompts sourced from src/prompts.py\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate datasets for all 3 prediction targets\n",
    "prediction_targets = [\"delta_only\", \"postop_only\", \"binary_only\"]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Dataset Generation - Fully Integrated with src/prompts.py\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "print(\"This notebook uses the following functions from src/prompts.py:\")\n",
    "print(\"  1. get_system_prompt() - System message\")\n",
    "print(\"  2. construct_user_prompt_sharegpt() - User message\")\n",
    "print(\"  3. create_ground_truth_response() - Assistant message\")\n",
    "print(\"\")\n",
    "print(\"NO prompt text is hardcoded in this notebook.\")\n",
    "print(\"To modify any prompt, edit src/prompts.py and re-run.\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for target in prediction_targets:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing dataset for: {target}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    dataset = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for idx, row in metadata_df.iterrows():\n",
    "        patient_id = str(row['CaseNum'])\n",
    "        \n",
    "        entry = process_patient_for_target(patient_id, row, target)\n",
    "        \n",
    "        if entry is not None:\n",
    "            dataset.append(entry)\n",
    "            if len(dataset) % 50 == 0:\n",
    "                print(f\"Processed {len(dataset)} patients...\")\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    \n",
    "    # Save dataset\n",
    "    output_path = OUTPUT_FILES[target]\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(dataset, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n✓ Saved {len(dataset)} samples to {output_path}\")\n",
    "    print(f\"  Skipped {skipped_count} patients due to missing data\")\n",
    "    \n",
    "    # Show sample entry\n",
    "    if len(dataset) > 0:\n",
    "        print(f\"\\n  Sample entry structure:\")\n",
    "        print(f\"  - Number of images: {len(dataset[0]['images'])}\")\n",
    "        print(f\"  - Number of messages: {len(dataset[0]['messages'])}\")\n",
    "        print(f\"  - System message length: {len(dataset[0]['messages'][0]['content'])} chars\")\n",
    "        print(f\"  - User message length: {len(dataset[0]['messages'][1]['content'])} chars\")\n",
    "        print(f\"  - Assistant response: {dataset[0]['messages'][2]['content'][:100]}...\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"All datasets generated successfully!\")\n",
    "print(\"All prompts sourced from src/prompts.py\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying dataset format...\n",
      "\n",
      "delta_only:\n",
      "  Total samples: 287\n",
      "  Sample validation:\n",
      "    - Has 'messages' field: True\n",
      "    - Has 'images' field: True\n",
      "    - Messages count: 3 (should be 3: system, user, assistant)\n",
      "    - Images count: 12\n",
      "    - Message roles: ['system', 'user', 'assistant']\n",
      "    - First image path: ../../data/preprocessed/1/MR_T2S/004.png\n",
      "    - System prompt preview: \n",
      "<PROMPT>\n",
      "<ROLE>\n",
      "You are an expert clinical reasoning AI specializing in neurosurgery, radiology, an...\n",
      "    - Assistant response: {\"rationale\": \"Based on the provided imaging and clinical data, the predicted JOA change is -1.0.\", \"change\": -1}...\n",
      "\n",
      "postop_only:\n",
      "  Total samples: 287\n",
      "  Sample validation:\n",
      "    - Has 'messages' field: True\n",
      "    - Has 'images' field: True\n",
      "    - Messages count: 3 (should be 3: system, user, assistant)\n",
      "    - Images count: 12\n",
      "    - Message roles: ['system', 'user', 'assistant']\n",
      "    - First image path: ../../data/preprocessed/1/MR_T2S/004.png\n",
      "    - System prompt preview: \n",
      "<PROMPT>\n",
      "<ROLE>\n",
      "You are an expert clinical reasoning AI specializing in neurosurgery, radiology, an...\n",
      "    - Assistant response: {\"rationale\": \"Based on the provided imaging and clinical data, the predicted post-operative JOA score is 14.0.\", \"score\": 14}...\n",
      "\n",
      "binary_only:\n",
      "  Total samples: 287\n",
      "  Sample validation:\n",
      "    - Has 'messages' field: True\n",
      "    - Has 'images' field: True\n",
      "    - Messages count: 3 (should be 3: system, user, assistant)\n",
      "    - Images count: 12\n",
      "    - Message roles: ['system', 'user', 'assistant']\n",
      "    - First image path: ../../data/preprocessed/1/MR_T2S/004.png\n",
      "    - System prompt preview: \n",
      "<PROMPT>\n",
      "<ROLE>\n",
      "You are an expert clinical reasoning AI specializing in neurosurgery, radiology, an...\n",
      "    - Assistant response: {\"rationale\": \"Based on the provided imaging and clinical data, the patient is not expected to achieve good functional recovery.\", \"improved\": false}...\n",
      "\n",
      "✓ Format verification complete!\n",
      "\n",
      "============================================================\n",
      "SUMMARY: Complete Integration with src/prompts.py\n",
      "============================================================\n",
      "\n",
      "This notebook contains:\n",
      "  ✓ Data loading logic\n",
      "  ✓ File I/O operations\n",
      "  ✗ NO prompt text (all in src/prompts.py)\n",
      "\n",
      "To modify prompts:\n",
      "  1. Edit src/prompts.py\n",
      "  2. Re-run this notebook\n",
      "  3. Done! Changes automatically reflected\n",
      "\n",
      "Functions used from src/prompts.py:\n",
      "  - get_system_prompt()\n",
      "  - construct_user_prompt_sharegpt()\n",
      "  - create_ground_truth_response()\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset format\n",
    "print(\"\\nVerifying dataset format...\\n\")\n",
    "\n",
    "for target, output_path in OUTPUT_FILES.items():\n",
    "    with open(output_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"{target}:\")\n",
    "    print(f\"  Total samples: {len(data)}\")\n",
    "    \n",
    "    if len(data) > 0:\n",
    "        sample = data[0]\n",
    "        print(f\"  Sample validation:\")\n",
    "        print(f\"    - Has 'messages' field: {('messages' in sample)}\")\n",
    "        print(f\"    - Has 'images' field: {('images' in sample)}\")\n",
    "        print(f\"    - Messages count: {len(sample['messages'])} (should be 3: system, user, assistant)\")\n",
    "        print(f\"    - Images count: {len(sample['images'])}\")\n",
    "        print(f\"    - Message roles: {[msg['role'] for msg in sample['messages']]}\")\n",
    "        print(f\"    - First image path: {sample['images'][0]}\")\n",
    "        print(f\"    - System prompt preview: {sample['messages'][0]['content'][:100]}...\")\n",
    "        print(f\"    - Assistant response: {sample['messages'][2]['content'][:150]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"✓ Format verification complete!\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY: Complete Integration with src/prompts.py\")\n",
    "print(\"=\"*60)\n",
    "print(\"\")\n",
    "print(\"This notebook contains:\")\n",
    "print(\"  ✓ Data loading logic\")\n",
    "print(\"  ✓ File I/O operations\")\n",
    "print(\"  ✗ NO prompt text (all in src/prompts.py)\")\n",
    "print(\"\")\n",
    "print(\"To modify prompts:\")\n",
    "print(\"  1. Edit src/prompts.py\")\n",
    "print(\"  2. Re-run this notebook\")\n",
    "print(\"  3. Done! Changes automatically reflected\")\n",
    "print(\"\")\n",
    "print(\"Functions used from src/prompts.py:\")\n",
    "print(\"  - get_system_prompt()\")\n",
    "print(\"  - construct_user_prompt_sharegpt()\")\n",
    "print(\"  - create_ground_truth_response()\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dcm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
